{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_dir = \"../../results/experiment-results\"\n",
    "fold_dir_template = \"fold={}\"\n",
    "evaluation_file_template = \"Model{}_validation_epoch{:03d}_results.csv\"\n",
    "\n",
    "folds = [1, 2, 3, 4, 5]\n",
    "model_types = [\"CNN\", \"GCN\"]\n",
    "epochs = [e for e in range(1, 101)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2anomalyid = {0: 0, 1: 2, 2: 5}\n",
    "anomalyid2idx = {v:k for k,v in idx2anomalyid.items()}\n",
    "prob_cols = [\"prob_node_anomaly_{}\".format(v) for v in idx2anomalyid.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(fold, model_type, epoch):\n",
    "    fold_dir = fold_dir_template.format(fold)\n",
    "    evaluation_file = evaluation_file_template.format(model_type, epoch)\n",
    "    path = os.path.join(experiment_results_dir, fold_dir, evaluation_file)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(labels, predictions, prefix=\"mc\"):\n",
    "    scores = {\n",
    "        f\"{prefix}_acc\": accuracy_score(labels, predictions),\n",
    "        f\"{prefix}_acc_balanced\": balanced_accuracy_score(labels, predictions),\n",
    "        f\"{prefix}_f1_avg\": f1_score(labels, predictions, average='macro'),\n",
    "        f\"{prefix}_f1_wavg\": f1_score(labels, predictions, average='weighted'),\n",
    "        f\"{prefix}_prec_avg\": precision_score(labels, predictions, average='macro'),\n",
    "        f\"{prefix}_prec_wavg\": precision_score(labels, predictions, average='weighted'),\n",
    "        f\"{prefix}_rec_avg\": recall_score(labels, predictions, average='macro'),\n",
    "        f\"{prefix}_rec_wavg\": recall_score(labels, predictions, average='weighted'),\n",
    "        f\"{prefix}_confusion_matrix\": confusion_matrix(labels, predictions)\n",
    "    }\n",
    "    \n",
    "    for i, s in enumerate(f1_score(labels, predictions, average=None)):\n",
    "        scores[f\"{prefix}_f1_{idx2anomalyid[i]}\"] = s\n",
    "    for i, s in enumerate(precision_score(labels, predictions, average=None)):\n",
    "        scores[f\"{prefix}_prec_{idx2anomalyid[i]}\"] = s\n",
    "    for i, s in enumerate(recall_score(labels, predictions, average=None)):\n",
    "        scores[f\"{prefix}_rec_{idx2anomalyid[i]}\"] = s\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    df_scores = pd.DataFrame()\n",
    "    for model_type, epoch in product(model_types, epochs):\n",
    "        path = get_path(fold, model_type, epoch)\n",
    "        df = pd.read_csv(path)\n",
    "        df[\"true_graph_anomaly_idx\"] = df[\"true_node_anomaly\"].apply(lambda val: val) # simple copy\n",
    "        df[\"true_graph_anomaly\"] = df[\"true_node_anomaly\"].apply(lambda val: val) # simple copy\n",
    "        df[\"prediction\"] = df[prob_cols].apply(lambda row: idx2anomalyid[np.argmax(row.to_numpy())], axis=1)\n",
    "        \n",
    "        group_df = df[[\"true_graph_anomaly\", \"true_graph_anomaly_idx\", \"prediction\", \"file_id\", \"sequence_id\"]].groupby([\"file_id\"]).agg({\n",
    "            \"true_graph_anomaly\": lambda col: col.values.ravel()[np.flatnonzero(col.values)].tolist(),\n",
    "            \"true_graph_anomaly_idx\": lambda col: np.flatnonzero(col.values).tolist(),\n",
    "            \"prediction\": list,\n",
    "            \"sequence_id\": max\n",
    "        })\n",
    "                \n",
    "        graph_true_labels = group_df[\"true_graph_anomaly\"].values.tolist()\n",
    "        graph_true_label_indices = group_df[\"true_graph_anomaly_idx\"].values.tolist()\n",
    "        node_pred_labels = group_df[\"prediction\"].values.tolist()\n",
    "        sequence_ids = group_df[\"sequence_id\"].values.tolist()\n",
    "        file_ids = group_df.index.tolist()\n",
    "                        \n",
    "        labels = df[\"true_node_anomaly\"].to_numpy()\n",
    "        predictions = df[\"prediction\"].to_numpy()\n",
    "        \n",
    "        # scores for multi-class\n",
    "        mc_scores = calculate_scores(labels, predictions, prefix=\"mc\")\n",
    "        \n",
    "        # scores for binary-class\n",
    "        labels[labels > 0] = 1\n",
    "        predictions[predictions > 0] = 1\n",
    "        bc_scores = calculate_scores(labels, predictions, prefix=\"bc\")\n",
    "        \n",
    "        scores = {**mc_scores, **bc_scores}\n",
    "        \n",
    "        scores[\"fold\"] = fold\n",
    "        scores[\"model_type\"] = model_type\n",
    "        scores[\"epoch\"] = epoch\n",
    "        scores[\"graph_true_labels\"] = graph_true_labels\n",
    "        scores[\"graph_true_label_indices\"] = graph_true_label_indices\n",
    "        scores[\"node_pred_labels\"] = node_pred_labels\n",
    "        scores[\"file_ids\"] = file_ids\n",
    "        scores[\"sequence_ids\"] = sequence_ids\n",
    "\n",
    "        df_scores = df_scores.append(scores, ignore_index=True)\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with Pool(len(folds)) as p:\n",
    "    dfs_scores = p.map(run, folds)\n",
    "\n",
    "df_scores = pd.concat(dfs_scores, ignore_index=True)\n",
    "print(len(df_scores))\n",
    "df_scores.to_csv(f\"../../results/calculated_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
